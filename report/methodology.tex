\section{Methodology}

\subsection{Test Environment}

\begin{figure}
    \input{figures/test_setup}
    \caption{Experiment setup}
    \label{fig:setup}
\end{figure}

\begin{table}
    \centering
    \begin{tabular}{l|l}
        \hline
        \hline
        Manufacturer    & Hardkernel\\
        Platform        & ODROID-X2\\
        SoC             & Samsung Exynos 4412 "Prime"\\
        CPU Core        & ARM Cortex-A9 (r3p0)\\
        Number of Cores & 4\\
        Clock Freq.     & $1.7$ GHz\\
        Core Voltage    & $1.3V$\\
        OS              & Debian GNU/Linux Testing (``jessie'')\\
        Kernel          & Linux 3.8 (custom)\\
        \hline
        \hline
        Voltmeter       & Agilent 34410A\\
        Power Supply    & Agilent E3631A\\
        Shunt Resistor  & Thermovolt AB 5697 0002 12m$\Omega$\\
        \hline
        \hline
    \end{tabular}
    \caption{System specifications}
    \label{table:system_spec}
\end{table}

In our experiments, we are using the ODROID-X2 \cite{odroid-x2} developer
platform, which has an Exynos 4412 "Prime" System-on-Chip with four ARM
Cortex-A9 processor cores. We disable three of the cores through \emph{sysfs}, leaving only
one core available to the scheduler. The processor runs at a fixed frequency of
$1.7$ GHz. The test environment is sketched in \autoref{fig:setup} and the details are summarized in
\autoref{table:system_spec}.


The Cortex-A9 is a 32-bit out-of-order dual-issue speculative RISC processor,
and even though its primary use is in mobile and embedded applications, it
shares many features with current desktop processors \cite{patterson,hennessy}.
It can issue two instructions per cycle and branches its pipeline into four
lanes, as depicted in \autoref{fig:pipeline}. Most instructions can
execute in either of the two general ALU's, but multiply instructions must
execute in the ALU with a hardware multiplier. The processor core also has
separate units for floating point operations (the NEON co-processor) and address
manipulation, but these will not be further considered in this paper.

\begin{figure}
    \begin{centering}
        \includegraphics[width=0.48\textwidth]{figures/A9-Pipeline-hres}
        \caption{ARM Cortex-A9 pipeline and peripherals\cite{a9whitepaper}}
        \label{fig:pipeline}
    \end{centering}
\end{figure}

In general, energy consumption of a processor varies with respect to the
workload; the harder it has to work, the more energy it uses. In this paper, we
seek to achieve the highest possible ALU throughput the processor can offer. To
accomplish this, we are required to gain knowledge of the pipeline and other
components within the CPU.

Official documentation of the pipeline structure is limited to the ``Cortex-A9
Technical Reference Manual'' \cite{armtech} and ``The ARM Cortex-A9 Processors''
whitepaper \cite{a9whitepaper}. However, by running some architectural
experiments and consulting the performance counters we are able to
infer some details.

\rowcolors{2}{gray!25}{white}
\subsection{Architectural Experiments}
\label{arch_experiments}
The A9 processor has 58 distinct events\footnote{A complete overview can be seen
in table A.18 in \cite{armtech}} that
each can be mapped to one of six generic event counters in the Performance
Monitor Unit (i.e. only six generic events can be tracked simultaneously). It
also has a separate cycle counter. By comparing execution unit counters for the
two ALUs and the cycle counter, we obtain detailed statistics about the pipeline
activity. For example, we run \texttt{add} instructions with and without
hazards, and verify that the core is able to share the work between execution
units in the latter case. Note that these performance counters are approximate
due to the speculative core, and only serves as a guideline and sanity check for
our assumptions.

Using performance counters as above, we are able to confirm a feature on the A9
processor that is very vaguely documented; fast-loop mode. As the name suggests,
this feature enables rapid execution of small loops. It does so by fetching
instructions from the instruction cache only at the first loop iteration,
effectively voiding time and energy spent on instruction lookups between
iterations. However, which loops that falls into this category is not
documented, but by using performance counters we are able to determine this with
confidence. We disable the L1 cache, penalizing runs that do not fit in
fast-loop, making it easy to distinguish between runs within and outside
fast-loop. We find that for a loop to be executed in fast-loop it must hold one
subtle property: the loop in its entirety must fit within the first 60 bytes of
a 64 byte cache line. Consequently, the loop body must be 13 instructions or
less (15 including \texttt{sub} and \texttt{bne}). Loops without this property
will cause code to be executed outside fast-loop and get a significant decrease
in performance.

Furthermore, executing code within fast-loop limits the number of cache
mispredicts to two, independent of the iteration count. We confirm this by
looking at the cache mispredict performance counter.

\subsection{Benchmarks}
As a first approximation, the benchmark programs consists of an infinite series
of identical instructions. Since the A9 core runs at a fixed frequency and we are
providing a fixed core voltage, so energy usage per instruction can
be deduced from the continuous power drain during instruction runs. In general,
the power drain related to a particular instruction can be calculated as following:

\begin{equation}
    P_{instruction} = I_{instruction} \cdot V_{core}
    \label{eq:1}
\end{equation}

The fixed core clock cycle gives a fixed amount of time per cycle. Thus, we
use clock cycles as our base time unit. Energy is then given as

\begin{equation}
    E_{instruction} = P_{instruction} \cdot cycles
    \label{eq:2}
\end{equation}

We are unable to measure core power directly with our equipment. However,
voltage and frequency are assumed constant, which gives a linear relation
between current and consumed energy. Instead of providing numbers in Joule per
instruction, we measure the current drain and multiply with the number of cycles
the instruction lies within any of the processors functional units. This gives
us the unit of Ampere-cycles, which in our environment maps directly to energy.
We neglect the voltage drop over the shunt resistor, which is in the order of a
few mV.

This simple setup does not take the memory system into account; we are
undoubtedly not able to feed the processor instructions at no cost in terms of
access speed and -- more importantly -- memory system energy usage. Thus, we
enhance our setup by running all benchmark code within fast-loop. To explicitly
feed the processor instructions without the overhead of interrupts, we write
Linux kernel modules that once inserted, execute a loop similar to the one shown
in \autoref{list:inst_loop}. Note that the \texttt{subs} and \texttt{bne} are
used to generate a loop body small enough to fit in fast-loop, and at the same
time allows us to terminate the program after some number of iterations.

It is stated in \cite{armtech} that branching to immediate locations does not
use clock cycles. Our micro-benchmarks branches to immediate locations, but it
does so conditionally. We assume that the calculation of the branch condition,
the \texttt{subs}, takes its normal execution time, while the following branch
instruction is invisible.

\begin{figure}
    \begin{lstlisting}{language=[ARM]Assembler}
    label:
    instruction
    ... ; repeats 13X
    instruction
    subs
    bne label
    \end{lstlisting}
    \caption{Instruction loop}
    \label{list:inst_loop}
\end{figure}

\subsection{Power Measurements}
To measure energy consumption, we use an Agilent 34410A
multimeter\cite{agilent34410a} to sense the voltage drop over a shunt
resistor, set up as shown in \autoref{fig:setup}. The multimeter is
configured to sample at its highest sampling rate of 10 kHz. This yields one
sample every 170,000 instructions with an error of at most $1 mV$. It is
obvious that we are unable to observe inter-cycle fluctuations with this
equipment, but as we run the same instruction practically indefinitely we
extract the average. The loop for each instruction runs for about 20 seconds and
we gather 50,000 samples (over a period of 5 seconds) in the middle of this loop.
Observational errors are accounted for by running the power measurement loop
many times for each instruction. We also sleep 30 seconds in between runs to
dilute the effect of temperature variations. Running over the entire testbench
takes about 100 minutes and we average the medians for each instruction run to
get a single value.

We separate power consumption on the ARM cores and the development board by
modifying the ODROID-X2 and providing a separate power supply for the A9 cores.
They get powered by an external power supply giving $1.3V$ DC, while the rest of
the board is powered from a another power supply at $5.0V$, as depicted in
\autoref{fig:setup}. We cannot verify that CPU cores sit alone on the $1.3V$
power rail, but we observe a strong degree of correlation between core activity
and $V_{core}$ power drain.

Certain instructions use more than one cycle to complete their work, so the
energy usage has to be normalized. An instruction that occupies the pipeline for
two cycles is believed to use approximately twice as much energy. By
normalizing, we can convert point-in-time current drain in terms of Amperes
to energy per instruction in Ampere-cycles.

\subsection{Pitfalls}
We measure the current drain of different instructions separately, so we need
to fix as many parameters as possible. We must acknowledge that some factors
affects power consumption and produces noise in our data.

\label{sec:temperature}
One obvious such factor is the chip temperature: it is known that power
consumption increases at higher core temperatures. We explore the boundaries by
physically applying cooling spray and notice that our measurements on average
gets 4\% higher with a temperature increase from 9$^\circ$C to 63$^\circ$C . The
\texttt{mul} instruction had the greatest leap and used 7\% more energy at
63$^\circ$C. In our experiments, only one of the four available cores are
used.  Stressing a single core over time did not increase temperature by more
than 7$^\circ$C (from idle at 47$^\circ$C to 54$^\circ$C at load) and reached
an equilibrium where temperature remained constant. Assuming that it is
generally true that a single core cannot heat the entire SoC significantly, and
that the increase in power consumption is at most 10\% over 50$^\circ$C, we get

\begin{equation}
    P_{inc} = P_{orig} \cdot T_{inc} \cdot \frac{0.10}{50} = P_{orig} \cdot T_{inc} \cdot 0.002
\end{equation}

Assuming the trend is close to linear, output will increase by 0.2\% per
$^\circ$C increased. Also, we start our measurements several seconds after the
benchmarks, giving the core plenty of time to reach work temperature. For our
purpose, the time used to reach work temperature was pretty much instant. Note
that this temperature logging was done with a different kernel as it required
support for Dynamic Voltage and Frequency Scaling (DVFS), which we
disabled in the test setup in order to fix the clock frequency.

Energy consumption is almost certainly affected by the amount of bit flipping
within the core. In all the tests, the instruction arguments are static. This
means that the results could be different if we changed the arguments. To
mitigate this, we used as equal arguments as possible. Still, different
instructions contain and use arguments differently, so we cannot guarantee
complete fairness between instructions.

We are running Linux as the base environment for our tests, which makes it
simpler to run our micro-benchmarks. However, running an entire operating
system beneath our benchmark programs implies that there is much going on where
we have no direct control. To mitigate the artifacts originating from
the operating system, we disable all the maskable interrupts and run our
benchmark programs entirely uninterrupted as a kernel module.

As explained in section \autoref{arch_experiments}, we utilize the fast-loop
mode of the processor to avoid memory access latency. We disable the L1 cache
to easier detect when we are outside the fast-loop mode, and thus we are
certain that there is no memory access going on.
