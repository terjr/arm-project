\section{Methodology}

\subsection{Test Environment}
\begin{table}
    \begin{tabular}{l|l}
        \hline
        \hline
    Manufacturer    & Hardkernel\\
    Platform        & Odroid-X2\\
    SoC             & Samsung Exynos 4412 "Prime"\\
    CPU Core        & ARM Cortex-A9\\
    Number of cores & 4\\
    Clock Freq.     & 1.7GHz\\
    Core Voltage    & 1.3V\\
    OS              & Debian testing ``jessie''\\
    Kernel          & 3.8 custom\\
        \hline
        \hline
    Voltmeter       & Agilent 34410A\\
    Power supply    & Agilent E3631A\\
    Shunt resistor  & 12m$\Omega$\\
        \hline
        \hline
    \end{tabular}
    \caption{System specifications}
    \label{table:system_spec}
\end{table}

In our experiments, we are using the ODROID-X2 \cite{odroid-x2} developer
platform, which has an Exynos 4412 "Prime" System-on-Chip with four ARM
Cortex-A9 (r3p0) processor cores. We disable three of the cores, leaving only
one core configured to run at a fixed frequency of $1.7$ GHz. The test
environment is sketched in \autoref{fig:setup}, while the details are summarized in
\autoref{table:system_spec}.

\begin{figure}
    \input{figures/test_setup}
    \caption{Experiment Setup}
    \label{fig:setup}
\end{figure}

The Cortex-A9 is a 32-bit out-of-order dual-issue speculative RISC processor,
and even though its primary use is in mobile and embedded applications, it
shares many features with current desktop processors. \cite{patterson}
\cite{hennessy}. It can issue two instructions per cycle and branches its
pipeline to four functional units, as depicted in \autoref{fig:pipeline}. Most
instructions can execute in either of the two general ALU's, but multiply
instructions must execute in the ALU with a hardware multiplier. The processor
core also has separate units for floating point operations (the NEON
co-processor) and address manipulation, but will not be further considered in
this paper.

\begin{figure}
    \begin{centering}
        \includegraphics[width=0.48\textwidth]{figures/A9-Pipeline-hres}
        \caption{ARM Cortex-A9 Pipeline and peripherals,\hfill
        figure from ARM Cortex-A9 Whitepaper\cite{a9whitepaper}}
        \label{fig:pipeline}
    \end{centering}
\end{figure}

In general, energy consumption of a processor varies with respect to the
workload; the harder it has to work, the more energy it uses. In this paper, we
seek to achieve the highest possible ALU throughput the processor can offer. To
accomplish this, we are required to gain knowledge of its pipeline and other
components within the CPU.

Official documentation of the pipeline structure is limited to the ``Cortex-A9
Technical Reference Manual'' \cite{armtech} and ``The ARM Cortex-A9 Processors''
whitepaper \cite{a9whitepaper}. However, by running some architectural
experiments and consulting the performance counters we are able to
infer some details.

\rowcolors{2}{gray!25}{white}
\subsection{Architectural Experiments}
\label{arch_experiments}
The A9 processor has 58 distinct events\footnote{A complete overview can be seen
in table A.18 in the Cortex-A9 Technical Reference Manual \cite{armtech}} that
each can be mapped to one of six generic event counters in the Performance
Monitor Unit (i.e. only six generic events can be tracked simultaneously). It
also has a separate cycle counter. By comparing execution unit counters for the
two ALUs and the cycle counter, we obtain detailed statistics about the
pipeline activity. For example, we run \texttt{add} instructions with and
without hazards, and verify that counters for the hazard-less run completes
twice in about half the number of cycles. Note that these performance counters are
approximate due to speculativity in the core, and is only being used as a sanity
check for what we expected.

\begin{table}
    \centering
    \begin{tabular}{|p{0.7cm}|R{0.8cm}R{0.6cm}R{0.9cm}R{0.6cm}R{0.6cm}R{0.8cm}R{0.8cm}|}
       \rowcolor{gray!50}
        \hline
        \centering
        Instr. &
        \centering
        Cycles &
        \centering
        Main Ex. &
        \centering
        Second Ex. &
        \centering
        Pred. &
        \centering
        Mis pred. &
        \centering
        No disp. &
        \begin{centering}
        Issue Empty
        \end{centering}
        \\
        \hline
        \input{table_mul}
        \hline
    \end{tabular}
    \caption{Performance counter data from 252 iterations of all tested multiply
    instructions.}
    \label{tab:perf_mul}
\end{table}

\begin{table}
    \centering
    \begin{tabular}{|p{0.7cm}|R{0.8cm}R{0.6cm}R{0.9cm}R{0.6cm}R{0.6cm}R{0.8cm}R{0.8cm}|}
       \rowcolor{gray!50}
        \hline
        \centering
        Instr. &
        \centering
        Cycles &
        \centering
        Main Ex. &
        \centering
        Second Ex. &
        \centering
        Pred. &
        \centering
        Mis pred. &
        \centering
        No disp. &
        \begin{centering}
        Issue Empty
        \end{centering}
        \\
        \hline
        \input{table_instr}
        \hline
    \end{tabular}
    \caption{Performance counter data from 252 iterations of all tested
    instructions, excluding multiply.}
    \label{tab:perf_nonmul}
\end{table}


Using performance counters as above, we are able to confirm a feature on the A9
processor that is very vaguely documented; fast-loop\texttrademark{} mode. As
the name suggests, this feature enables rapid execution of small loops. It does
so by fetching instructions from the icache only at the first loop iteration,
effectively voiding time and energy spent on instruction cache lookups between
iterations. However, which loops that falls into this category is not
documented, but by using performance counters we are able to determine this with
confidence. We disable the L1 cache, penalizing runs that do not fit into
fast-loop, making it easy to distinguish between runs within and outside
fast-loop. We find that for loops to be executed in fast-loop it must keep two
premises. First, the loop body must have 15 instructions or less. Secondly, it
must be aligned to a 64 byte cache line, i.e. the first instruction in the loop
must be on a 64-byte aligned address. Violating any of these two properties
will cause code to be executed outside fast-loop and yield a significant
decrease in performance.

Furthermore, executing code within fast-loop limits the number of cache
miss predicts to two independent of the iteration count. We confirm this by
looking at the cache miss predict performance counter. The first miss is likely to
occur at the end of the first iteration, while the second occurs on the way out
of the loop.

\subsection{Benchmarks}
As a first approximation, the benchmark programs consists of an infinite series
of identical instructions. The A9 core runs at a fixed frequency and we are
providing a fixed core voltage, so energy usage (in Watts) for a one-cycle
instruction is determined by the following formula.

\begin{equation}
    P_{instruction} = A_{instruction} \cdot V_{core}
    \label{eq:1}
\end{equation}

Running for a fixed time period, we obtain an expression for the energy used.

\begin{equation}
    E_{instruction} = P_{instruction} \cdot cycles
    \label{eq:2}
\end{equation}

This simple setup does not take the memory system into account; we are
undoubtedly not able to feed the processor instructions at no cost in terms of
access speed and -- more importantly -- memory system energy usage. Thus, we
enhance our setup by running all benchmark code within fast-loop. To explicitly
feed the processor instructions without the overhead of interupts, we write
Linux kernel modules that once inserted, execute a loop similar to the one shown
in \autoref{list:inst_loop}. Note that the \texttt{subs} and \texttt{jne}
instructions are needed to avoid memory fetches completely, but also makes the
program terminate -- which again eases the testing procedure.

The technical manual states that branching to immediate locations
does not consume execution unit cycles. Our microbenchmarks branches to
immediate locations, but it does so conditionally. We assume that the
calculation of this condition takes normal execution time, but that the branch
is invisible.

\begin{figure}
    \begin{lstlisting}{language=[ARM]Assembler}
    label:
    instruction
    ... ; repeats 13X
    instruction
    subs
    jne label
    \end{lstlisting}
    \caption{Instruction loop}
    \label{list:inst_loop}
\end{figure}

\subsection{Power Measurements}
To measure energy consumption, we use an Agilent 34410A
multimeter\cite{agilent34410a} to sense the voltage drop over a negligible
12m$\Omega$ resistor, set up as shown in \autoref{fig:setup}. The multimeter is
configured to sample at full precision at its maximum rate of 1000 Hz. This
gives one sample every 1.7 million instructions with an error of at most 0.002V.
It is obvious that we are unable to observe inter-cycle fluctations with this
equipment, but as we run the same instruction practically indefinetely we get an
average. The instruction loop runs for about 20 seconds for each instruction and
we use 5 seconds in the middle of this period to gather 5000 samples.

Observational errors are accounted for by running the power measurement loop
many times for each instruction. We also sleep 30 seconds in between
instructions to diminish the effect of temperature variations. Running over all
tested instructions typically takes 3 hours and we average the medians for each
instruction run to get a single value.

We separate power consumption on the ARM cores and the development board by
modifying the ODROID-X2 and providing a separate power supply for the A9 cores.
They get powered by an external power supply giving $1.3V$ DC, while the rest of
the board is powered from a another power supply at $5.0V$, as depicted in
\autoref{fig:setup}.

Some instructions takes a different amount of time, the power drain has to
be normalized using statistics gained from performance counters. An instruction
that occupies the pipeline for two cycles, is believed to use approximately
twice as much energy. By normalizing, we can convert point-in-time energy
consumption in terms of Watts to energy per instruction (in Joules).


\subsection{Pitfalls}
% temperature, noise (inducted power, etc.), interrupts, memory latency
% (fast-loop)
We are comparing the energy efficiency of different instructions in an
asynchronous way, so we try to fixate as many parameters as possible. We must
acknowledge that some factors affects power consumption and produces noise in
our data.

\label{sec:temperature}
One obvious such factor is the chip temperature: it is known that power
consumption increases at higher core temperatures. We explore the boundaries by
physically applying freeze spray and notice that our measurements gets 4\%
higher with a temperature increase from 9 to 63 Celsius. The \texttt{mul}
instruction had the greatest leap and used 7\% more energy at 63 degrees. In our
experiments, only one of the four available cores are used. Stressing a single
core over time did not increase temperature by more than 7 degrees (from idle at
47$^\circ$ C to 54$^\circ$ C at load), so the chip is evidently able to
dissipate more heat than one core can generate. Assuming that it is generally
true that a single core cannot heat the entire SoC significantly,
and that the increase in power consumption is at most 10\% over 50$^\circ$
Celsius, we get

\begin{equation}
    P_{inc} = P_{orig} \cdot T_{inc} \cdot \frac{0.10}{50} = P_{orig} \cdot T_{inc} \cdot 0.002
\end{equation}

Believing that this trend is at least close to linear, output will increase by
0.2\% pr. degree Celsius increased. Also, we start our measurements several
seconds after the benchmarks itself starts, giving the core plenty of time to
gain work temperature. In our tests, the time used to get to work temperature
was humanly instant. Note that we could not log temperature from on-chip sensors
while running our tests, as these sensors are tightly coupled with DVFS in the
Linux kernel, which we have disabled.

Another factor that is not that obvious, but equally important, is power
induced in the measurement circuit. Since wires are often winded up on the test
bench, and lab equipment might contain large metal cores with a great amount of
power running through them, unexpected power might be introduced.

Running Linux as the base environment for our tests gives us a much simpler
abstractions to make the processor run our programs, but it also comes with some
caveats. Having an entire operating system beneath our benchmark programs
implies that there is much going on that we have no direct controls over. In
order to mitigate the artifacts originating from the operating system, we
disable all the maskable interrupts and run our benchmark programs entirely
uninterupted.

As explained in section \autoref{arch_experiments}, we utilize the fast-loop mode of
the processor to mitigate memory access latency. We disable the L1 cache to
easier detect when we are outside the fast-loop mode, and thus we are certain
that there is no memory access going on.
